% \chapter{Simultaneous Localization and Mapping}\label{chap:slam}
\chapter{同步定位与建图（SLAM）}\label{chap:slam}
% Robots are able to keep track of their position using a model of the noise arising in their drive train and their forward kinematics to propagate this error into a spatial probability density function (Section \ref{sec:errorprop}). The variance of this distribution can shrink as soon as the robot sees uniquely identifiable features with known locations. This can be done for discrete locations using Bayes' rule (Section \ref{sec:markovloc}) and for continuous distributions using the Extended Kalman Filter (Section \ref{sec:ekf}). The key insight here was that every observation will reduce the variance of the robot's position estimate. Here, the Kalman filter performs an optimal fusion of two observations by weighting them with their variance, i.e., unreliable information counts less than reliable one. In the robot localization problem, one of the observations is typically the robot's position estimate whereas the other observation comes from a feature with known location on a map. So far, we have assumed that these locations are known. This chapter will introduce

\textit{机器人能够使用他们的传动系列中出现的噪声模型及其正向运动来跟踪其位置，以将该误差传播到空间概率密度函数}（第\ref{sec:errorprop}节））。一旦机器人观测到位置已知的唯一可识别特征，该分布的方差就会缩小。对于离散位置可以使用贝叶斯规则（第\ref{sec:markovloc}节），对于连续分布可以使用扩展卡尔曼滤波器（第\ref{sec:ekf}节）。其主要观点是，每次观测都会减少机器人位置估计的方差。这里，卡尔曼滤波器通过对它们的方差进行加权来执行两个观测值的最佳融合，即不可靠的信息权重小于可靠信息。在机器人定位问题中，其中一个观测通常是机器人的位置估计，而另一个观测来自地图上已知位置的特征。到目前为止，我们假设这些位置是已知的。本章将介绍

% \begin{itemize}
% \item the concept of covariance (or, what all the non-diagonal elements in the covariance matrix are about),
% \item how to estimate the robot's location and that of features in the map at the same time (Simultaneous Localization and Mapping or SLAM)
% \end{itemize}

\begin{itemize}
\item 协方差概念（或协方差矩阵中的所有非对角元素的内涵）
\item 如何同时估计地图中的机器人和特征的位置（同步定位与建图或SLAM）
\end{itemize}


% \section{Introduction}
% The SLAM problem has been considered as the holy grail of mobile robotics for a long time. This lecture will introduce one of the first comprehensive solutions to the problem, which has now be superseded by computationally more efficient versions. We will begin with studying a series of special cases.

\section{引言}

SLAM问题长期以来被认为是移动机器人的圣杯。本讲将介绍问题的第一种综合解决方案之一，它现在已经被计算上更有效的版本所取代了。我们从研究一系列特殊情况开始。

% \subsection{Special Case I: Single Feature}
% Consider a map that has only a single feature. We assume that the robot is able to obtain the relative range and angle of this feature, each with a certain variance. An example of this and how to calculate the variance of an observation based on sensor uncertainty is described in the line fitting example (Section \ref{sec:linefitting}). This feature could be a wall, but also a graphical tag that the robot can uniquely identify. The position of this measurement $m_i=[\alpha_i,r_i]$  in global coordinates is unknown, but can now easily be calculated if an estimate of the robot's position $\boldsymbol{\hat{x}_k}$ is known.  The variance of $ m_i$'s components is now the variance of the robot's position plus the variance of the observation.

\subsection{特殊情况一：单一特征}

考虑一个只有一个特征的地图。我们假设机器人能够获得该特征的相对距离和角度，这两个值都有一定的方差。线性拟合示例（第\ref{sec:linefitting}节）描述了这种情况，以及如何计算基于传感器不确定度的观测值的方差。该特征可以是墙壁，也可以是机器人可以唯一识别的图行标签。 这个测量的位置$m_i = [\alpha_i，r_i]$的全局坐标是未知的，但是如果机器人的位置$\boldsymbol {\hat{x}_k}$的估计是已知的，那么就可以很容易地计算出。$m_i$的组件的方差现在是机器人位置的方差加上观测的方差。

% Now consider the robot moving closer to the obstacle and obtaining additional observations. Although its uncertainty in position is growing, it can now rely on the feature $m_i$ to reduce the variance of its old position (as long as its known that the feature is not moving). Also, repeated observations of the same feature from different angles might improve the quality of its observation. The robot has therefore a chance to keep its variance very close to that with which it initially observed the feature and stored it into its map. We can actually do this using the EKF framework from Section \ref{sec:EKF}. There, we assumed that features have a known location (no variance), but that the robot's sensing introduces a variance. This variance was propagated into the covariance matrix of the innovation ($ \boldsymbol{S}$). We can now simply add the variance of the estimate of the feature's position to that of the robot's sensing process.

现在考虑机器人靠近障碍物并获得更多观测。 虽然它的不确定性在不断增加，但现在可以依靠特征$ m_i $来减少其旧位置的方差（只要它已知该特征是静止的）。 此外，从不同角度重复观测相同的特征可能会提高其观测质量。这样机器人就可能使方差非常接近初始观测该特征的方差，并将该特征存储到地图中。 我们可以使用第\ref{sec:EKF}节中的EKF框架真正地做到。当时，我们假设特征位置已知（无方差），但机器人的观测会引入方差。这个方差被传播到创新的协方差矩阵（$\boldsymbol{S}$）。我们现在可以简单地将特征位置的估计的方差加到机器人的感知过程中。

% \subsection{Special Case II: Two Features}
% Consider now a map that has two features. Visiting one after the other, the robot will be able to store both of them in its map, although with a higher variance for the feature observed last. Although the observations of both features are independent from each other, the relationship between their variances depend on the trajectory of the robot. The differences between these two variances are much lower if the robot connect them in a straight line than when it performs a series of turns between them. In fact, even if the variances of both features are huge (because the robot has already driven for quite a while before first encountering them), but the features are close together, the probability density function over their distance would be very small. The latter can also be understood as the covariance of the two random variables (each consisting of range and angle). In probability theory, the covariance is the measure of how much two variables are changing together. Obviously, the covariance between the locations of two features that are visited immediately after each other by a robot is much higher as those far apart. It should therefore be possible to use the covariance between features to correct estimates of features in retrospect. For example, if the robot returns to the first feature it has observed, it will be able to reduce the variance of its position estimate. As it knows that it has not traveled very far since it observed the last feature, it can then correct this feature's position estimate.

\subsection{特殊情况二：两个特征}

现在考虑一个有两个特征的地图。机器人先后访问这两个特征，并将它们存储在地图中，但后观测到的特征具有较高的方差。虽然对这两个特征的观测是相互独立的，但它们的方差之间的关系取决于机器人的轨迹。机器人通过直线运动访问的两个方差之间的差异，要比在它们之间进行一系列转弯时的差异要小得多。事实上，即使两个特征的方差都很大（因为机器人在首次遇到前已经开了很长时间了），但如果两个特征距离很近，两者距离上的概率密度函数就会很小。后者也可以理解为两个随机变量的协方差（每个变量由距离和角度组成）。在概率论中，协方差是两个变量变化相关度的量度。显然，机器人先后访问的两个相距很近的特征的位置之间的协方差，远远高于那些相距较远的位置之间的协方差。因此，我们可以使用特征之间的协方差来校正对特征的估计。例如，如果机器人返回到其观测到的第一个特征，则能够减少对其位置估计的方差。由于它知道自从观测到上一个特征以来，它没有走过很远，所以它可以纠正这个特征的位置估计。

% \section{The Covariance Matrix}
% When estimating quantities with multiple variables, such as the position of a robot that consists of its x-position, its y-position and its orientation, matrix notation has been a convenient way of writing down equations. For error propagation, we have written the variances of each input variable into the diagonal of a covariance matrix. For example, when using a differential wheel robot, uncertainty in position expressed by $ \sigma_x, \sigma_y$ and $ \sigma_{\theta}$ were grounded in the uncertainty of its left and right wheel. We have entered the variances of the left and right wheel into a 2x2 matrix and obtained a 3x3 matrix that had $ \sigma_x, \sigma_y$ and $ \sigma_{\theta}$ in its diagonal. Here, we set all other entries of the matrix to zero and ignored entries in the resulting matrix that were not in its diagonal. The reason we could actually do this is because uncertainty in the left and right wheel are independent random processes: there is no reason that the left wheel slips, just because the right wheel slips.  Thus the covariance --- the measure on how much two random variables are changing together --- of these is zero. This is not the case for the robot's position: uncertainty in one wheel will affect all output random variables ($ \sigma_x, \sigma_y$ and $ \sigma_{\theta}$) at the same time, which is expressed by their non-zero covariances --- the non-zero entries off the diagonal of the output covariance matrix.


\section{协方差矩阵}

当使用多个变量估计数量时，例如由x位置，y位置及其角度组成的机器人的位置，矩阵符号已成为写下方程式的便捷方式。对于误差传播，我们将每个输入变量的方差写入所得协方差矩阵的对角线。例如，当使用差动轮机器人时，基于其左右轮的不确定性，我们用$\sigma_x$，$\sigma_y $和$\sigma_{\theta}$表示的位置的不确定性。我们已经将左右轮的方差进入了一个$2\times2$矩阵，并获得了一个$3\times3$矩阵，它的对角线中有$\sigma_x$，$\sigma_y$和$\sigma_{\theta}$。这里，我们将矩阵的所有其他项设置为零，并忽略不在其对角线的矩阵中的项。我们可以这样做的原因是左右轮的不确定性是独立的随机过程：左轮打滑并不仅仅是因为右轮打滑。因此，这些项的协方差---对两个随机变量变化相关度的度量---是零。机器人的位置不是这样：一个轮子的不确定性将同时影响所有的输出随机变量（$\sigma_x$，$\sigma_y $和$\sigma_{\theta}$），即所得协方差矩阵的对角线的非零项。

\section{EKF SLAM}\label{sec:ekfslam}\label{sec:ekf}
% The key idea in EKF SLAM is to extend the state vector from the robot's position to contain the position of all features. Thus, the state
EKF SLAM的关键思路是将状态向量从机器人的位置扩展到包含所有特征的位置。 因此，状态
\begin{equation}
\hat{\boldsymbol{x}}_{k'|k-1}=(x,y,\theta)^T
\end{equation}
% becomes
变为
\begin{equation}
\hat{\boldsymbol{x}}_{k}=(x,y,\theta,\alpha_1,r_1,\ldots,\alpha_N,r_N)^T
\end{equation}

% assuming $ N$ features, which is a $(3+2N) x1$ vector. The action update (or ``prediction update") is identical to that if features are already known; the robot simply updates its position using odometry and updates the variance of it s position using error propagation. The covariance matrix is now a $(3+2N)x(3+2N)$ matrix that initially holds the variances on position and those of each feature in its diagonal.

如果假设有$ N $个特征，那么这是一个$（3+2N） \times 1 $的向量。 如果特征已知，那么运动更新（或“预测更新”）和它是等价的；机器人使用测距仪简单更新其位置，并使用误差传播更新其位置的方差。协方差矩阵现在是$（3 + 2N）\times（3 + 2N）$矩阵，包括其最初保持位置上的方差和其对角线上每个特征的方差。

% The interesting things happen during the perception update. Here it is important that only one feature is observed at a time. Thus, if the robot observes multiple features at once, one needs to do multiple, consecutive perception updates. Care needs to be taken that the matrix multiplications work out. In practice you will need to set only those values of the observation vector (a $(3+2N)x1$ vector) that correspond to the feature that you observe. Similar considerations apply to the observation function and its Jacobian.

观测更新期间产生有趣的事情。需要强调的是一次只能观测到一个特征。因此，如果机器人一次性观测多个特征，则需要进行多个连续的观测更新。 需要注意矩阵乘法运算。 在实践中，只需要设置与观测到的特征对应的观测向量（$（3 + 2N）\times 1 $向量）的值。 类似的方法适用于观测函数及其雅可比。

% \section{Graph-based SLAM}
\section{基于图的SLAM}

% Usually, a robot obtains an initial estimate of where it is using some onboard sensors (odometry, optical flow, etc.) and uses this estimate to localize features (walls, corners, graphical patterns) in the environment. As soon as a robot revisits the same feature twice, it can update the estimate on its location. This is because the variance of an estimate based on two independent measurements will always be smaller than any of the variances of the individual measurements. As consecutive observations are not independent, but rather closely correlated, the refined estimate can then be propagated along the robot's path. This is formalized in EKF-based SLAM. A more intuitive understanding is provided by a spring-mass analogy: each possible pose (mass) is constrained to its neighboring pose by a spring. The higher the uncertainty of the relative transformation between two poses (e.g., obtained using odometry), the weaker the spring. Every time a robot gains confidence on a relative pose, the spring is stiffened instead. Eventually, all poses will be pulled in place. This approach is known as \emph{Graph-based SLAM}\index{Graph-based SLAM}. 

通常，机器人使用某些机载传感器（odometry，optical flow等）获得它所在位置的初始估计，并使用该估计来定位环境中的特征（墙壁，角落，图图案）。一旦机器人重新访问相同的特征，它可以更新对其位置的估计。这是因为基于两个独立测量的估计的方差总是小于任何单独测量的方差。由于连续的观测结果不是独立的，而是紧密相关的，这个改进的估计可以沿着机器人的访问路径传播。基于EKF的SLAM对它进行公式化。通过弹簧质量类比可以提供更直观的理解：每个可能的姿势（质量）被弹簧约束到其相邻姿态。两个姿势（例如，使用测距法获得）之间相对转换的不确定性越高，弹簧越弱。相反，每当机器人对相对姿态的确定性增强时，弹簧会变硬。最终，所有姿势都将被拉到位。这种方法被称为\emph{基于图的SLAM}\index{基于图的SLAM}。

% \subsection{SLAM as a Maximum-Likelihood Estimation Problem}
\subsection{SLAM作为最大似然估计问题}

% The classical formulation of SLAM describes the problem as maximizing the posterior probability of all points on the robot's trajectory given the odometry input and the observations. Formally,

SLAM的经典公式将SLAM问题描述成给定测距输入和观测值的情况下最大化机器人轨迹上所有点的后验概率。形式上，
\begin{equation}
p(x_{1:T},m|z_{1:T},u_{1:T})
\end{equation}
% where $ x_{1:T}$ are all discrete positions from time 1 to time $ T$, $ z$ are the observations, and $ u$ are the odometry measurements. This formulation makes heavily use of the temporal structure of the problem. In practice, solving the SLAM problem requires

其中$ x_{1:T} $是从时间$1$到$T$的离散位置，$z$是观测值，而$u$是距离测量值。这个表述大量使用问题的时间结构。 在实践中，解决SLAM问题需要

% \begin{enumerate}
% \item A motion update model, i.e., the probability $ p(x_t|x_{t-1},u_t)$ to be at location $ x_t$ given an odometry measurement $ u_t$ and being at location $ x_{t-1}$.
% \item  A sensor model, i.e., the probability $ p(z_t|x_t,m_t)$ to make observation $ z_t$ given the robot is at location $ x_t$ and the map $ m_t$.
% \end{enumerate}

\begin{enumerate}
\item 运动更新模型，即给定距离测量值$u_t$及位置$ x_{t-1}$，位置$x_t$的概率$ p(x_t | x_{t-1}，u_t)$。
\item 传感器模型，即已知机器人在位置$x_t$和地图$m_t$，作出观测$z_t$的概率$ p(z_t | x_t，m_t)$。
\end{enumerate}

% A possible solution to this problem is provided by the Extended Kalman Filter, which maintains a probability density function for the robot pose as well as the positions of all features on the map. Being able to uniquely identify features in the environment is of outmost importance and is known as the data association problem. Like EKF-based SLAM, graph-based SLAM does not solve this problem and will fail if features are confused.

扩展卡尔曼滤波器是解决这个问题的一个可能的方案，它保持机器人姿态的概率密度函数以及地图上所有特征的位置。 能够唯一地识别环境中的特征是非常重要的，被称为数据关联问题。如果特征混淆，像基于EKF的SLAM一样，基于图的SLAM也不能解决这个问题，也会失败。

% In graph-based SLAM, a robot's trajectory forms the nodes of a graph whose edges are transformations (translation and rotation) that have a variance associated with it. An alternative view is the spring-mass analogy mentioned above. Instead of having each spring wiggle a node into place, graph-based SLAM aims at finding those locations that maximize the joint likelihood of all observations. As such, graph-based SLAM is a \emph{maximum likelihood estimation}\index{Maximum Likelihood Estimation} problem.

在基于图的SLAM中，机器人的轨迹形成了图的点，而图的边则是其相联点的转换（平移和旋转）。另一种看法则是上面提到的弹簧质量类比。基于图的SLAM旨在找到最大化所有观测的联合可能性的那些位置，而不是使每个弹簧摆动一个点到位。 因此，基于图的SLAM是\emph{最大似然估计}\index{最大似然估计}问题。

% Lets revisit the normal distribution:

让我们回顾正态分布：

\begin{equation}
\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}
\end{equation}

% It provides the probability for a measurement to have value $ x$ given that this measurement is normal distributed with mean $ \mu$ and variance $ \sigma^2$.  We can now associate such a distribution with every node-to-node transformation, aka constraint. This can be pairs of distance and angle, e.g. In the literature the measurement of a transformation between node i and a node j is denoted $ z_{ij}$. Its expected value is denoted $ \hat{z}_{ij}$. This value is expected for example based on a map of the environment that consists of previous observations.

假定测量是正态分布的，它为测量值$ x $提供了平均值为$\mu$和方差$ \sigma^2 $的概率描述。我们现在可以将这样的分布与每个点到点的转换，即约束。比如，可以是距离和角度对。在文献中，点$i$和点$j$之间的变换的测量被表示为$ z_{ij} $。 其期望值表示为$ \hat{z}_{ij} $，比如基于由先前观测结果组成的环境地图。

% Formulating a normal distribution of measurements $ z_{ij}$ with mean $ \hat{z}_{ij}$ and a covariance matrix $ \Sigma_{ij}$ (containing all variances of the components of $ z_{ij}$ in its diagonal) is now straightforward. As graph-based SLAM is most often formulated as  information filter, usually the inverse of the covariance matrix (aka information matrix) is used, which we denote by $ \Omega_{ij}=\Sigma_{ij}^{-1}$.

现在可以直接公式化测量值$ z_{ij} $的正态分布，其平均值为$ \hat{z}_{ij} $，协方差矩阵为$ \Sigma_{ij} $（对角线包含$ z_{ij} $的组件的所有变量）。基于图的SLAM最常被公式化为信息过滤器，我们通常使用协方差矩阵（也称为信息矩阵）的逆，表示为$ \Omega_{ij} = \Sigma_{ij}^{-1} $。

% As we are interested in maximizing the joint probability of all measurements $ \prod{z_{ij}}$ over all edge pairings $ ij$ following the maximum likelihood estimation framework, it is customary to express the PDF using the log-likelihood. By taking the natural logarithm on both sides of the PDF expression, the exponential function vanishes and $ ln \prod{z_{ij}}$ becomes $ \sum{ ln z_{ij}}$ or $ \sum{l_{ij}}$, where $ l_{ij}$ is the log-likelihood distribution for $ z_{ij}$.

基于最大似然估计框架，我们想最大限度地提高所有测量值$ \prod{z_{ij}} $的所有边缘配对$ ij $的联合概率，所以习惯上使用对数似然表示$PDF$。 对$PDF$表达式两边取自然对数，指数函数消失，$ln\prod{z_{ij}} $变为$ \sum{ln z_{ij}} $或$ \sum{l_{ij} } $，其中$ l_{ij} $是$ z_{ij} $的对数似然分布。

\begin{equation}
l_{ij} \propto (z_{ij}-\hat{z}_{ij}(x_i,x_j))^T\Omega_{ij}(z_{ij}-\hat{z}_{ij}(x_i,x_j))
\end{equation}

% Again, the log-likelihood for observation $ z_{ij}$ is directly derived from the definition of the normal distribution, but using the information matrix instead of the covariance matrix and is ridden of the exponential function by taking the logarithm on both sides.

再次，观测的对数似然度$ z_{ij} $直接来源于正态分布的定义，我们使用信息矩阵而不是协方差矩阵，并通过对两边取对数去掉指数函数。

% The optimization problem can now be formulated as
这个优化问题现在可以表示为
\begin{equation}
x^* = \arg \min_{x}\sum_{<i,j>\in \mathcal{C}}e_{ij}^T\Omega_{ij}e_{ij}
\end{equation}
% with $ e_{ij}(x_i,x_j)=z_{ij}-\hat{z}_{ij}(x_i,xj)$ the error between measurement and expected value. Note that the sum actually needs to be minimized as the individual terms are technically the negative log-likelihood.

$ e_{ij}(x_i，x_j)= z_ {ij} - \hat{z}_{ij}(x_i，xj)$为测量值与期望值之间的误差。请注意，实际上需要将总和最小化，因为个别术语在技术上是负的对数似然。

% \subsection{Numerical Techniques for Graph-based SLAM}
\subsection{基于图SLAM的数值技术}

% Solving the MLE problem is non-trivial, especially if the number of constraints provided, i.e., observations that relate one feature to another, is large. A classical approach is to linearize the problem at the current configuration and reducing it to a problem of the form $ Ax=b$. The intuition here is to calculate the impact of small changes in the positions of all nodes on all $ e_{ij}$. After performing this motion, linearization and optimization can be  repeated until convergence. 

解决MLE问题是不平凡的，特别是如果提供的约束数量很大，即将一个特征与另一个特征相关联的观测值。一种经典的方法是在当前设置下线性化该问题，并将其化简为$ Ax = b $形式的问题。直觉是计算所有点对所有$ e_{ij} $位置的微小变化的影响。 化简后，可以重复线性化和优化直至收敛。

% Recently, more powerful numerical methods have been developed. Instead of solving the MLE, one can employ a stochastic gradient descent algorithm. A gradient descent algorithm is an iterative approach to find the optimum of a function by moving along its gradient. Whereas a gradient descent algorithm would calculate the gradient on a fitness landscape from all available constraints, a stochastic gradient descent picks only a (non-necessarily random) subset. Intuitive examples are fitting a line to a set of $n$ points, but taking only a subset of these points when calculating the next best guess. As gradient descent works iteratively, the hope is that the algorithm takes a large part of the constraints into account. For solving Graph-based SLAM, a stochastic gradient descent algorithm would not take into account all constraints available to the robot, but  iteratively work on one constraint after the other. Here, constraints are observations on the mutual pose of nodes $i$ and $j$. Optimizing these constraints now requires moving both nodes $i$ and $j$ so that the error between where the robot thinks the nodes should be and what it actually sees gets reduced.  As this is a trade-off between multiple, maybe conflicting observations, the result will approximate a Maximum Likelihood estimate.

最近，有更强大的数值方法被研究出来。可以采用随机梯度下降算法，而不是求解MLE。梯度下降算法是通过沿其梯度移动来找到函数的最优值的迭代方法。梯度下降算法利用所有可用约束计算值域上的梯度，而随机梯度下降仅选择（不一定随机的）一部分。直观的例子是对$ n $个点进行直线拟合，但是只利用一部分点计算下一个最佳猜测。随着梯度下降反复运行，我们希望算法用到大部分约束。为了解决基于图的SLAM，随机梯度下降算法不会利用机器人可用的所有约束，而是迭代地一个约束一个约束地计算。在这里，约束是对点$ i $和$ j $的相互姿态的观测。现在优化这些约束需要移动两个点$ i $和$ j $，以便机器人认为点之间的误差和实际观测的误差会减少。由于这是在多个可能冲突的观测结果之间的折中，所以结果逼近最大似然估计。

% More specifically, with $ e_{ij}$ the error between an observation and what the robot expects to see, based on its previous observation and sensor model, one can distribute the error along the entire trajectory between both features that are involved in the constraint. That is, if the constraint involves features $i$ and $j$, not only $i$ and $j$'s pose will be updated but all points inbetween will be moved a tiny bit.

更具体地说，根据其先前的观测和传感器模型，使用$ e_{ij} $观测值与机器人期望观测的误差，可以向两个特征之间沿着整个轨迹涉及的约束中分散误差。 也就是说，如果约束涉及到特征$ i $和$ j $，不仅$ i $和$ j $的姿态会被更新，中间的所有点也都会移动一点点。

%% Start - Was Commented
%This approach is cumbersome and quickly gets out of control if a robot is mapping an environment over multiple hours --- leading to millions of nodes in the graph and constraints. To overcome this problem, [Gris07] propose to (1) merge nodes of a graph as it is build up by relying on accurate localization of the robot within the existing map and (2) to chose a different graph representation.
%% End - Was Commented


% In Graph-based SLAM, edges encode the relative translation and rotation from one node to the other. Thus, altering a relationship between two nodes will automatically propagate to all nodes in the network. This is because the graph is essentially a chain of nodes whose edges consist of odometry measurements. This chain then becomes a graph whenever observations (using any sensor) introduce additional constraints. Whenever such a ``loop-closure'' occurs, the resulting error will be distributed over the entire trajectory that connects the two nodes. This is not always necessary, for example when considering the robot driving a figure-8 pattern. If a loop-closure occurs in one half of the 8, the nodes in the other half of the 8 are probably not involved.


在基于图的SLAM中，边对从一个点到另一个点的相对平移和旋转编码。因此，改变两个点之间的关系将自动传播到图中的所有点。这是因为图本质上是一个点链，其边由距离测量值组成。每当观测（使用任何传感器）引入额外的约束时，该链就变成图。 每当发生这种“闭环”时，产生的误差将在连接两个点的整个轨迹上分散。这并不总是必需的，例如在考虑机器人驾驶图8图案时。 如果在8的一半产生“闭环”，那么8的另一半中的点可能没有涉及。


% This can be addressed by constructing a  minimum spanning-tree  (MST) of the constraint graph. The MST is constructed by doing a Depth-First Search (DFS) on the constraint graph following odometry constraints. At a loop-closure, i.e., an edge in the graph that imposes a constraint to a previously seen pose, the DFS backtracks to this node and continues from there to construct the spanning tree. Updating all poses affected by this new constraint still requires modifying all nodes along the path between the two features that are involved, but inserting additional constraints is greatly simplified. Whenever a robot observes new relationships between any two nodes, only the nodes on the shortest path between  the two features on the MST need to be updated. %Example graphs illustrating this are shown in Figures 2a and 2b in [Gris07].

这可以通过构建约束图的最小生成树（MST）来解决。 通过在约束图上执行深度优先搜索（DFS）来构建MST。在“闭环”（即，图中对边界施加约束到先前看到的姿态）的边缘，DFS回溯到该点并从那里继续构建生成树。更新受此新约束影响的所有姿态，当然还需要修改所涉及的两个特征之间路径上的所有点，但是插入其他约束被大大简化。每当机器人观测到任何两个点之间的新关系时，只需要更新MST上两个特征之间最短路径上的点。

% \subsection*{Further reading}
\subsection*{进一步阅读}
\begin{itemize}
\item G. Grisetti, R. Kuemmerle, C. Stachniss and W. Burgard. A Tutorial on Graph-Based SLAM. IEEE Intelligent Transportation Systems Magazine, 2(4):31-43, 2010.

\item E. Olson, J. Leonard and S. Teller. Fast Iterative Alignment of Pose Graphs with Poor Initial Estimates. Proc. of ICRA, pp 2262-2269, Orlando, FL, 2006.

\item G. Grisetti, C. Stachniss, S. Grzonka and W. Burgard. A Tree Parameterization for Efficiently Computing Maximum Likelihood Maps using Gradient Descent. Robotics: Science and Systems (RSS), Atlanta, GA, USA, 2007.
\end{itemize}
